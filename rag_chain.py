import os
from langchain.vectorstores import FAISS
from langchain.docstore.document import Document
from langchain_huggingface import HuggingFaceEmbeddings

# ‚úÖ Load product descriptions from local file
def load_documents(file_path="products.txt"):
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    return [Document(page_content=line.strip()) for line in lines if line.strip()]

# ‚úÖ Create embedding model
embedding_fn = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ‚úÖ Load or rebuild FAISS index
def get_vectorstore(index_dir="faiss_index", products_file="products.txt"):
    index_path = os.path.join(index_dir, "index.faiss")
    try:
        if os.path.exists(index_path):
            return FAISS.load_local(index_dir, embedding_fn, allow_dangerous_deserialization=True)
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to load FAISS index: {e}")

    # Rebuild index if loading fails
    print("üîÑ Rebuilding FAISS index from products.txt...")
    docs = load_documents(products_file)
    vectorstore = FAISS.from_documents(docs, embedding_fn)
    vectorstore.save_local(index_dir)
    return vectorstore

# ‚úÖ Initialize vectorstore
vectorstore = get_vectorstore()

# ‚úÖ Retrieval function
def retrieve_context(query, k=3):
    return vectorstore.similarity_search(query, k=k)

# ‚úÖ Test block for standalone execution
if __name__ == "__main__":
    test_queries = [
        "wireless headphones",
        "eco-friendly water bottle",
        "gaming laptop with SSD"
    ]

    for query in test_queries:
        print(f"\nüîé Query: {query}")
        results = retrieve_context(query)
        for i, doc in enumerate(results, 1):
            print(f"Result {i}: {doc.page_content}")
